---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# loadcell

`loadcell` package provides tools for cleaning and parsing haul and endline data from load cell vertical line data collected for the Maine Department of Marine Resources.

## Installing loadcell

`loadcell` is a non-CRAN package, and requires `devtools` for installation.

First, install `devtools` with the following command:

```{r eval=FALSE}
install.packages('devtools')
``` 

Then, install the load cell package directly from Bill's GitHub account with the following command:

```{r eval=FALSE}
## Install LoadCell from Bill's GitHub
devtools::install_github(repo = "bdevoe/loadcell")
``` 

## Basic function use

### Loading CSVs

A sample CSV of actual load cell data has been included in the package directory and will be used in this example. This package is intended to load CSVs generated from a separate load cell recording program, however any load CSV that matches the filename and field name conventions produced by this program could be used.

To load load cell CSV files from a directory, the `load_csvs` function can be used. The function will scan the current working directory, or optionally a directory can be passed with the `dir` argument. Directories are scanned recursively (CSVs in subdirectories will be included). In this example, my working directory is set to `example_data` in the package directory, so the single example CSV `17348465-2018.11.15-09.30.24-20.csv` will be loaded. The CSV is loaded to an object `csv_data` - using the `str` function will show the structure of this object. For more details, see the function documentation.

```{r eval=TRUE}
# Set wd to example_data folder
setwd("./example_data")
# Load loadcell
library(loadcell)
# Load CSV data
csv_data <- load_csvs()
# Examine
str(csv_data)
```

### Adjusting load values

Load values can be adjusted for the angle of the line through the block by applying a load multiplier.

```{r eval=TRUE}
# Adjust load values
csv_data_adj <- adjust_load(data = csv_data, kfactor = 0.6)
```


### Parsing Hauls

After CSVs are loaded, hauls can be parsed. The `parse_hauls` function handles this and has several options for how hauls should be delimited from load cell data. More details on these parameters is in the function help. In this example, `split = T` is being used to indicate the CSVs need to be split into separate hauls. If a fisherman created a separate CSV file for each haul, `split = F` would be used to simply number the hauls.

```{r eval=TRUE}
# Parse hauls from LoadCellData class object
haul_data <- parse_hauls(csv_data_adj, split = T, # Split hauls within CSV
                         min_load = 40, # Remove data < 40LBF
                         min_time = 30, # Minimum 30 seconds haul length
                         min_gap = 30, # Minimum 30 seconds between hauls
                         pass = 30) # If a CSV is less than 30 seconds long,
                                    # assume it is a single haul and do not split
# Examine the first haul structure
str(haul_data[[1]])
```

Looking at the structure of `haul_data`, note that the single CSV has been split into 15 separate hauls based on the defined arguments.

### Peak Analysis

Peaks analysis of haul data can be done with the `parse_peaks` function. This function will attempt to conduct peak analysis on each haul in an object of class `LoadCellHauls` created by the `parse_hauls` function. For more details on the arguments that can be input into this function, see the function help.

The goal of peak analysis is to identify the first major peak in the data that would typically occur when the first trap in a trawl reached the hauler. This first peak represents that maximum load on the vertical line that occured for the haul.

```{r eval=TRUE}
# Parse peaks from LoadCellHauls class object
peaks <- parse_peaks(data = haul_data, 
                     span = .05, # Percent smoothing to apply
                     peakdist = 10, # Minimum 10 samples between peaks
                     peakheight = 200) # Minimum peak height of 200 LBF
# Examine third haul peak analysis
str(peaks[[3]])
```

### Plotting data

Peaks can now be plotted with the `plot_hauls` function. Plots can be output to either the console or a PDF file. In this example, the hauls will be output to a PDF in the package folder called `TEST_PLOTS.pdf`

```{r eval=TRUE}
# Set wd to example_output folder
setwd("X:\\Bio Monitor\\LoadCells\\R\\loadcell\\example_output")
# Build plots for hauls
plots <- plot_hauls(data = peaks, fileout = "TEST_PLOTS")
```

### Exporting parsed data to CSVs

The `export_loadcell` function will export an object of class `LoadCellHauls` or `LoadCellPeaks` to a series of CSVs. A more in depth description of output is included in the function help. In this example, the data has been exported to a series of CSVs in the `example_output` folder in the package directory.

```{r eval=TRUE}
# Set wd to example_output folder
setwd("./example_output")
# Export
export_loadcell(peaks, prefix = "TEST")
```


## Piping

The provided functions also work with piping - here is the same example given from loading to export, but executed in a single pipe.

```{r eval=FALSE}
# Set wd to example_data folder
setwd("./example_output")
# Load loadcell
library(loadcell)
library(magrittr)
# Load CSV data
final_data <- load_csvs() %>%
  adjust_load(kfactor = 0.6) %>%
  parse_hauls(split = T,
              min_load = 40,
              min_time = 30,
              min_gap = 30,
              pass = 30) %>%
  parse_peaks(span = .05,
              peakdist = 10,
              peakheight = 200) %>%
  export_loadcell(prefix = "TEST")
  
```

